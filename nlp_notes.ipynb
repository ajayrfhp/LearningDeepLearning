{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJgIJ/UWdqaY/RvBkLAclJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajayrfhp/LearningDeepLearning/blob/main/nlp_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qM3jQ103MCQg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automated mixed precision\n",
        "- Implemented GPT2 training with torch.amp.autocast [here](https://github.com/ajayrfhp/GPT2/blob/master/notebooks/utils.py)\n",
        "  - Autocast automatically casts operations to move to FP16, which can remain in FP32. Idea is to save memory usage\n",
        "  - Main code modification is to apply loss scaling. Loss is scaled before backward prop. Small gradients can be rounded to 0, so you multiply with a large scaling factor. Gradients will stay above underflow threshold. Scale down before weight update.\n",
        "\n"
      ],
      "metadata": {
        "id": "xzDvzV-eMKGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers\n",
        "- Implemented GPT2 https://github.com/ajayrfhp/GPT2/\n",
        "  - Tested on toy datasets like repeating abcde\n",
        "  - Tried training on WikiText101 with 9 million tokens, 150 million model overfits. https://github.com/ajayrfhp/GPT2/blob/master/notebooks/GPT2OnWikiTextLarge.ipynb\n",
        "  - Wrote unit tests to check if model was implemented correctly.\n",
        "  - Was optimizing code to run on 90 million token wiki text, hit index error to fix.\n",
        "- Transformers don't need to concatenate context into a vector, passes the whole input matrix. Transformers are easier to parallelize. RNN process sequence by word. RNNs can't model long range sequences well.\n",
        "- We use layer norm, not batch norm because sequences can be of different length. RMSNorm, just divide by the square of variance, no need to subtract mean. Less FLOPS\n",
        "- Cost\n",
        "  - Compute\n",
        "    - Multiplying self attention matrix with input\n",
        "    - O (Batch_size * TokenLength * TokenLength * D)\n",
        "  - Memory\n",
        "    - Self attention matrices need to be stored. Longer sequences need more memory to be stored.\n",
        "    - O (Batch_size * TokenLength * TokenLength)\n"
      ],
      "metadata": {
        "id": "jcUClU1KNgxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "- Transformer models need input text broken into discrete tokens.\n",
        "- BPE is a common tokenization scheme, merge most commonly occuring 2 tokens into one and repeat till certain number of merges is achieved.\n",
        "  - Implemented BPE from scratch https://github.com/ajayrfhp/GPT2/blob/master/notebooks/bytepairencoding.ipynb\n",
        "  - Used copilot to write my code in cython and benchmark times show 66% faster processing time.\n",
        "  - BPE does not need UNK token, starts from character level.\n",
        "- SentencePiece does not use space for preprocessing, treats every unicode character as a seperate token and finds optimal token merge scheme that fits training data using EM https://github.com/ajayrfhp/GPT2/blob/master/notebooks/comparison_of_tokenizers.ipynb\n",
        "- WordPiece is older, split words into seperate pairs. Word piece merges pairs of subwords that maximize training data fit. Word piece has UNK token.\n",
        "- Vision transformer takes (w, h, c) image, applies a (k, k) kernel with stride k to get(w/k, h/k, d) then first two dimensions are flattened to get (w*h/kk) tokens each with d dimensional vector. Fed to transformers."
      ],
      "metadata": {
        "id": "tuq4VIwpO6KY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional embeddings\n",
        "- Resource https://huggingface.co/blog/designing-positional-encoding\n",
        "- Desired properties\n",
        "  - Unique encoding for each position\n",
        "  - Linear relationship between 2 encoded positions, relative\n",
        "  - Generalize to different sequences\n",
        "  - Deterministic mapping\n",
        "- Need in transformers because transformers lose positional information\n",
        "- Binary positional embedding\n",
        "  - Add binary representation of each position\n",
        "  - Transitions from one to next are not smooth\n",
        "- Fixed sinusoidal embedding\n",
        "  - Smoother transitions than binary positional encoding.\n",
        "  - Sin, cos functions of different frequencies are used to encode different types of relationships between positions.\n",
        "- Absolute positional embedding\n",
        "  - Learn embedding vector for each position. Works best if test sequences are of similar length to train sequences. Different dimensions encode position information captured in different frequencies\n",
        "- Relative positional embedding\n",
        "  - In language, meaning often comes from relation of words to other words\n",
        "  - Encode distance between 2 words.\n",
        "- ROPE\n",
        "  - Apply a rotational matrix to Query and Key vectors, where the rotational matrix encodes both rotation and distance between words.\n",
        "    - Each position is mapped to a unique rotation angle. x'=Rx\n",
        "    - Angle differences encode relative differences. The inner product of RMQ and RNK only depends on M - N\n",
        "    - As distance between word increases, attention weights decay. Easy implementation.\n",
        "- ALIBI\n",
        "  - A linear bias to attention scores is added based on distance between the words. A new head slope parameter is added.\n",
        "  -\n",
        "  "
      ],
      "metadata": {
        "id": "Thv_Xwu9QaXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO\n",
        "-\n",
        "Study the feedforward (MLP) block of transformers.\n",
        "understand the difference between allocated and reserved memory, transformer param count, memory consumption by attention and feedforward https://erees.dev/transformer-memory/\n",
        "parallel transformers: https://arxiv.org/pdf/2205.05198.pdf don't go too deep, try to understand the concepts like 1) how tensor parallel is implemented in feedforward, 2) how attention head and its fully connected layer can go parallel, 3) how sequence parallelism works"
      ],
      "metadata": {
        "id": "EBcgcFL9Wx04"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nRm5OaYsWxnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7P7xjUgoMJME"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}