{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5quwa5ZAT7s9Uyqevpalj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajayrfhp/LearningDeepLearning/blob/main/working_with_text_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRXbnlEDk_ku"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contents**\n",
        "- Prepare text for LLM training\n",
        "- Splitting text into word and subword tokens\n",
        "- Byte pair encoding\n",
        "- Sampling training examples\n",
        "- Convert tokens into vectors that go into LLM"
      ],
      "metadata": {
        "id": "NyCjNw_SlGy4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e1QEZ3CUvw2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GPT2 models have model size at 117M and 125M parameters, embedding size of 768.\n",
        "- GPT3 has 175B parameters and embedding size of 12,288."
      ],
      "metadata": {
        "id": "Z6uXVZBBlf3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize text"
      ],
      "metadata": {
        "id": "MuLKskIjpt93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import re\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "jE1yQgUlmWGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import read\n",
        "\n",
        "def download_data(url, file_path):\n",
        "  urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "def read_data(file_path):\n",
        "  with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    data = f.read()\n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "file_path = \"verdict.txt\"\n",
        "download_data(url, file_path)\n",
        "data = read_data(file_path)\n",
        "print(data[:1000])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvzd3Pl6lSJZ",
        "outputId": "4bacef36-7136-4524-b9cc-8b0863eadb50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
            "\n",
            "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
            "\n",
            "Well!--even through th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- White spaces are useful if we training models to generate code, but if we are operating pure text data, removing it is helpful for lowering memory and cpu constraints"
      ],
      "metadata": {
        "id": "Bd2qliTDnv-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "  tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "  tokens = [token for token in tokens if token.strip()]\n",
        "  return tokens\n",
        "\n",
        "tokens = tokenize_text(data)\n",
        "print(tokens[:30])\n",
        "print(len(tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C0fCGVimqMR",
        "outputId": "4ae2dc08-94dd-4076-bf2f-607c1e246105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n",
            "4690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert tokens into integers"
      ],
      "metadata": {
        "id": "NhOxlm4Tpzk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenizerV1:\n",
        "  def __init__(self, text):\n",
        "    self.text = text\n",
        "    self.tokens = self.tokenize_text(self.text)\n",
        "    print(f\"Text tokenized {len(self.tokens)}\")\n",
        "    self.word_to_idx = defaultdict(lambda: len(self.word_to_idx))\n",
        "    self.idx_to_word = defaultdict(lambda : \"UNK\")\n",
        "    self.build_vocab()\n",
        "    print(f\"Vocab size: {self.vocab_size} constructed\")\n",
        "\n",
        "\n",
        "  def tokenize_text(self, text):\n",
        "    tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    return [token for token in tokens if token.strip()]\n",
        "\n",
        "\n",
        "  def build_vocab(self):\n",
        "    self.all_words = sorted(set(self.tokens))\n",
        "    self.vocab_size = len(self.all_words)\n",
        "    for idx, word in enumerate(self.all_words):\n",
        "      self.word_to_idx[word] = idx\n",
        "      self.idx_to_word[idx] = word\n",
        "\n",
        "  def encode(self, text_to_be_encoded):\n",
        "    tokens = self.tokenize_text(text_to_be_encoded)\n",
        "    return [self.word_to_idx[token] for token in tokens]\n",
        "\n",
        "  def decode(self, encoded_text):\n",
        "    decoded_text = \" \".join([self.idx_to_word[idx] for idx in encoded_text])\n",
        "    print(decoded_text)\n",
        "    decoded_text = re.sub(r'\\s+([,.:;?_!\"()\\']])', r'\\1', decoded_text)\n",
        "    return decoded_text\n",
        "\n",
        "\n",
        "tokenizer = TokenizerV1(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd3_z6zqnDJQ",
        "outputId": "d7466235-c17b-4ce5-90b3-80539eca6749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text tokenized 4690\n",
            "Vocab size: 1130 constructed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"Jack is a genius\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tJkaRYvqRCl",
        "outputId": "e34467ee-d6f7-4838-d3f8-b5545f91f850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[57, 584, 115, 486]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(\"Jack is a genius\")) == \"Jack is a genius\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF73BScCthrB",
        "outputId": "b011f6da-d5e9-4b13-aa7d-4c4ace4ab42b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jack is a genius\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pinSywuHu7sQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}