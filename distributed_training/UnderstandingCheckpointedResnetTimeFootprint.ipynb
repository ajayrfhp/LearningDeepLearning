{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.24.5)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: matplotlib in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install matplotlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from torch.profiler import record_function\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set seed for reproducibility in torch, numpy and gpu\n",
    "\n",
    "torch.manual_seed(710)\n",
    "np.random.seed(710)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64>,\n",
       "  'label': 0},\n",
       " device(type='cuda', index=0))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to torch dataset\n",
    "\n",
    "class TinyImageNet(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx][\"image\"], self.dataset[idx][\"label\"]\n",
    "        # convert x to RGB\n",
    "        x = x.convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        y = torch.tensor(y, dtype=torch.int64)\n",
    "        return x, y\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "tiny_imagenet = load_dataset(\"Maysee/tiny-imagenet\", split=\"train\")\n",
    "tiny_imagenet_torch = TinyImageNet(tiny_imagenet, transform=transform)\n",
    "num_classes = len(tiny_imagenet.features[\"label\"].names)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "tiny_imagenet[0], device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 11689512\n",
      "Model GPU usage: 44.69 MB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.memory._record_memory_history(\n",
    "       max_entries=10000\n",
    "   )\n",
    "\n",
    "model_gpu_usage_before = torch.cuda.memory_allocated(device)\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.to(device)\n",
    "\n",
    "model_gpu_usage_after = torch.cuda.memory_allocated(device)\n",
    "\n",
    "model_gpu_usage = model_gpu_usage_after - model_gpu_usage_before\n",
    "\n",
    "# print number of parameters in the model\n",
    "\n",
    "print(f\"Number of parameters in the model: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Model GPU usage: {model_gpu_usage / 1024**2:.2f} MB\")\n",
    "\n",
    "del model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare effect of checkpointing on memory usage for large batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, val_loader, epochs=1, lr=0.001, break_after_num_batches=None, title=\"\"):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    total_times = []\n",
    "\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "        schedule=torch.profiler.schedule(wait=0, warmup=0, active=6, repeat=1),\n",
    "        record_shapes=True,\n",
    "        with_stack=True,\n",
    "        profile_memory=True\n",
    "    ) as prof:\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                prof.step()\n",
    "\n",
    "\n",
    "                inputs, labels = batch\n",
    "                \n",
    "                with record_function(\"to_device\"):\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                with record_function(\"forward\"):\n",
    "                    outputs = model(inputs)\n",
    "                \n",
    "                with record_function(\"backward\"):\n",
    "                    criterion(outputs, labels).backward()\n",
    "                \n",
    "                with record_function(\"optimizer_step\"):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                end_time = time.time()\n",
    "                # total_times.append(end_time - start_time)\n",
    "                \n",
    "                if break_after_num_batches is not None and batch_idx >= break_after_num_batches:\n",
    "                    break\n",
    "                start_time = time.time()\n",
    "    \n",
    "    total_times = np.array(total_times)\n",
    "    total_times = np.convolve(total_times, np.ones(rolling_window)/rolling_window, mode='valid')\n",
    "    mean_time = total_times.mean()\n",
    "\n",
    "                \n",
    "\n",
    "    prof.export_memory_timeline(f\"{title}_memory.html\", device=\"cuda:0\")\n",
    "    return {\n",
    "        \"mean_time\": mean_time,\n",
    "        \"total_times\": total_times\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "def clear_cuda_memory():\n",
    "    # Clear memory caches\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Reset peak memory stats\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Clear memory allocated by PyTorch\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Optional: Force garbage collection\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    # Print memory stats to verify\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"Cached memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "def fit_helper(model_type, dataset, epochs, break_after_num_batches, title):\n",
    "    clear_cuda_memory()\n",
    "    for batch_size in batch_sizes:\n",
    "        if model_type == \"resnet18_without_checkpointing\":\n",
    "            model = models.resnet18(pretrained=True)\n",
    "        elif model_type == \"resnet18_with_checkpointing\":\n",
    "            model = ResnetCheckpointed()\n",
    "        model.to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "        title += f\"_{batch_size}\"\n",
    "        oom_break = False\n",
    "\n",
    "        try:\n",
    "            times_dict = fit(model, train_loader, val_loader, epochs=1, break_after_num_batches=break_after_num_batches)\n",
    "            print(f\"Proccessed for batch size {batch_size}\")\n",
    "\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(batch_ids, data_load_times_dict['total_times'], label=f\"load time avg {data_load_times_dict['mean_time']}\", marker=\"o\", alpha=0.5)\n",
    "            plt.xlabel(\"Batch ID\")\n",
    "            plt.ylabel(\"Time (s)\")\n",
    "            plt.title(f\"load times with {num_workers} workers with avg total time {avg_total_time} and {checkpointing}\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(f\"Out of memory for batch size {batch_size}\")\n",
    "            oom_break = True\n",
    "        # clear memory\n",
    "        del model\n",
    "        del train_loader\n",
    "        del val_loader\n",
    "        clear_cuda_memory()\n",
    "        time.sleep(10)\n",
    "        if oom_break:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break down memory usage in Resnet\n",
    "- It takes memory to store the model, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResnetCheckpointed(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResnetCheckpointed, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Create a sequential container for the features\n",
    "        self.features = nn.Sequential(\n",
    "            self.model.conv1,\n",
    "            self.model.bn1,\n",
    "            self.model.relu,\n",
    "            self.model.maxpool,\n",
    "            self.model.layer1,\n",
    "            self.model.layer2,\n",
    "            self.model.layer3,\n",
    "            self.model.layer4,\n",
    "            self.model.avgpool\n",
    "        )\n",
    "        self.fc = self.model.fc\n",
    "        \n",
    "        # Number of segments to split the features into for checkpointing\n",
    "        self.segments = 3\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply checkpoint_sequential to features\n",
    "        x = checkpoint_sequential(self.features, self.segments, x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.00 MB\n",
      "Cached memory: 0.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/profiler/profiler.py:406: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
      "  warn(\"Profiler won't be using warmup, this can skew profiler results\")\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:545: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of memory for batch size 2048\n",
      "Allocated memory: 16.25 MB\n",
      "Cached memory: 6272.00 MB\n"
     ]
    }
   ],
   "source": [
    "num_workers = 4\n",
    "break_after_num_batches = 10\n",
    "batch_indices = None\n",
    "batch_sizes = [2048]\n",
    "\n",
    "\n",
    "fit_helper(\"resnet18_with_checkpointing\", tiny_imagenet_torch, 1, break_after_num_batches, title=\"with_checkpointing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding remarks\n",
    "- Without gradient checkpointing, we were able to fit upto batch size of 1024 in a single GPU \n",
    "- With gradient checkpointing, we were able to fit batch sizes of upto 4096 in a single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
