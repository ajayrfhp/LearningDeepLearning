{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Checkpointed ResNet Time Footprint\n",
    "## 1. Install and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: matplotlib in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from matplotlib) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from matplotlib) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install matplotlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from torch.profiler import record_function\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(710)\n",
    "np.random.seed(710)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImageNet(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx][\"image\"], self.dataset[idx][\"label\"]\n",
    "        x = x.convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        y = torch.tensor(y, dtype=torch.int64)\n",
    "        return x, y\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "tiny_imagenet = load_dataset(\"Maysee/tiny-imagenet\", split=\"train\")\n",
    "tiny_imagenet_torch = TinyImageNet(tiny_imagenet, transform=transform)\n",
    "num_classes = len(tiny_imagenet.features[\"label\"].names)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Memory Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W210 17:49:22.708481000 unwind.cpp:218] Warning: Unsupported unwinding pattern: Address not in range (function unwinderFor)\n",
      "/home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 11689512\n",
      "Model GPU usage: 44.69 MB\n"
     ]
    }
   ],
   "source": [
    "# Check baseline model memory usage\n",
    "torch.cuda.memory._record_memory_history(max_entries=10000)\n",
    "model_gpu_usage_before = torch.cuda.memory_allocated(device)\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.to(device)\n",
    "model_gpu_usage_after = torch.cuda.memory_allocated(device)\n",
    "model_gpu_usage = model_gpu_usage_after - model_gpu_usage_before\n",
    "print(f\"Number of parameters in the model: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Model GPU usage: {model_gpu_usage / 1024**2:.2f} MB\")\n",
    "del model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Checkpointed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class ResnetCheckpointed(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResnetCheckpointed, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Store individual layers\n",
    "        self.conv1 = self.model.conv1\n",
    "        self.bn1 = self.model.bn1\n",
    "        self.relu = self.model.relu\n",
    "        self.maxpool = self.model.maxpool\n",
    "        self.layer1 = self.model.layer1\n",
    "        self.layer2 = self.model.layer2\n",
    "        self.layer3 = self.model.layer3\n",
    "        self.layer4 = self.model.layer4\n",
    "        self.avgpool = self.model.avgpool\n",
    "        self.fc = self.model.fc\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply checkpointing to each layer\n",
    "        x = checkpoint(self.conv1, x)\n",
    "        x = checkpoint(self.bn1, x)\n",
    "        x = self.relu(x)  # ReLU is memory-efficient, no need to checkpoint\n",
    "        x = checkpoint(self.maxpool, x)\n",
    "        x = checkpoint(self.layer1, x)\n",
    "        x = checkpoint(self.layer2, x)\n",
    "        x = checkpoint(self.layer3, x)\n",
    "        x = checkpoint(self.layer4, x)\n",
    "        x = checkpoint(self.avgpool, x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training and Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, val_loader, epochs=1, lr=0.001, break_after_num_batches=None, title=\"\"):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    total_times = []\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "        schedule=torch.profiler.schedule(wait=0, warmup=0, active=6, repeat=1),\n",
    "        record_shapes=True,\n",
    "        with_stack=True,\n",
    "        profile_memory=True\n",
    "    ) as prof:\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                prof.step()\n",
    "                inputs, labels = batch\n",
    "                with record_function(\"to_device\"):\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                with record_function(\"forward\"):\n",
    "                    outputs = model(inputs)\n",
    "                with record_function(\"backward\"):\n",
    "                    criterion(outputs, labels).backward()\n",
    "                with record_function(\"optimizer_step\"):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                end_time = time.time()\n",
    "                total_times.append(end_time - start_time)\n",
    "                if break_after_num_batches is not None and batch_idx >= break_after_num_batches:\n",
    "                    break\n",
    "                start_time = time.time()\n",
    "    prof.export_memory_timeline(f\"{title}_memory.html\", device=\"cuda:0\")\n",
    "    batch_ids = np.arange(len(total_times))\n",
    "\n",
    "    total_times = np.array(total_times)\n",
    "    mean_time = np.round(np.mean(total_times), 2)\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(batch_ids, total_times, label=f\"Mean time: {mean_time} s\")\n",
    "    plt.xlabel(\"Batch ID\")\n",
    "    plt.ylabel(\"Time (s)\")\n",
    "    plt.title(f\"Training performance with gradient checkpointing\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def clear_cuda_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"Cached memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "def fit_helper(model_type, dataset, epochs, break_after_num_batches, num_workers, batch_sizes, title):\n",
    "    clear_cuda_memory()\n",
    "    for batch_size in batch_sizes:\n",
    "        if model_type == \"resnet18_without_checkpointing\":\n",
    "            net = models.resnet18(pretrained=True)\n",
    "        else:\n",
    "            net = ResnetCheckpointed()\n",
    "        net.to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "        oom_break = False\n",
    "        try:\n",
    "            times_dict = fit(net, train_loader, val_loader, epochs=epochs, break_after_num_batches=break_after_num_batches)\n",
    "            print(f\"Processed for batch size {batch_size}\")\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(f\"Out of memory for batch size {batch_size}\")\n",
    "            oom_break = True\n",
    "        del net\n",
    "        del train_loader\n",
    "        del val_loader\n",
    "        clear_cuda_memory()\n",
    "        time.sleep(10)\n",
    "        if oom_break:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.00 MB\n",
      "Cached memory: 0.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages/torch/profiler/profiler.py:488: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
      "  warn(\"Profiler won't be using warmup, this can skew profiler results\")\n",
      "/home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_workers = 2\n",
    "break_after_num_batches = 10\n",
    "batch_sizes = [128]\n",
    "fit_helper(\n",
    "    model_type=\"resnet18_with_checkpointing\", \n",
    "    dataset=tiny_imagenet_torch, \n",
    "    epochs=1, \n",
    "    break_after_num_batches=break_after_num_batches, \n",
    "    num_workers=num_workers, \n",
    "    batch_sizes=batch_sizes,\n",
    "    title=\"with_checkpointing\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions\n",
    "- Checkpointing can help fit larger batch sizes with limited GPU memory.\n",
    "- This notebook explores how it impacts time and memory usage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
