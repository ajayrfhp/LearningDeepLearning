{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "- Implement classification with resnet18 in tinyImagenet dataset using d2l library\n",
    "    - Wrap tiny imagenet data under d2l library\n",
    "    - Fit with GPU\n",
    "    - Record validation losses, accuracy, time taken to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import d2l.torch as d2l\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_workers = 2\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "device = d2l.try_gpu()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 64, 64]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "class TinyImagenetTorch(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx][\"image\"], self.dataset[idx][\"label\"]\n",
    "        x = x.convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        y = torch.tensor(y, dtype=torch.int64)\n",
    "        return x, y\n",
    "\n",
    "class TinyImagenetD2l(d2l.Module):\n",
    "    def __init__(self, batch_size, num_workers):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.train_data = load_dataset(\"Maysee/tiny-imagenet\", split=\"train\")\n",
    "        self.val_data = load_dataset(\"Maysee/tiny-imagenet\", split=\"valid\")\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((64, 64)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        data = self.train_data if train else self.val_data\n",
    "        return torch.utils.data.DataLoader(\n",
    "            TinyImagenetTorch(data, self.transform), \n",
    "            batch_size=self.batch_size, shuffle=train)\n",
    "    \n",
    "\n",
    "tiny_imagenet = TinyImagenetD2l(batch_size, num_workers)\n",
    "for x, y in tiny_imagenet.get_dataloader(train=True):\n",
    "    print(x.shape, y.shape)\n",
    "    break\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[247, 248, 250,  ..., 254, 254, 254],\n",
      "         [251, 250, 250,  ..., 254, 254, 254],\n",
      "         [254, 254, 252,  ..., 253, 253, 253],\n",
      "         ...,\n",
      "         [198, 238, 255,  ..., 222, 221, 217],\n",
      "         [245, 247, 230,  ..., 249, 245, 240],\n",
      "         [242, 238, 245,  ..., 254, 252, 252]],\n",
      "\n",
      "        [[253, 254, 254,  ..., 254, 254, 254],\n",
      "         [255, 254, 254,  ..., 254, 254, 254],\n",
      "         [255, 255, 254,  ..., 253, 253, 253],\n",
      "         ...,\n",
      "         [200, 240, 255,  ..., 222, 221, 217],\n",
      "         [255, 255, 241,  ..., 249, 248, 243],\n",
      "         [255, 255, 255,  ..., 254, 255, 255]],\n",
      "\n",
      "        [[251, 252, 253,  ..., 254, 254, 254],\n",
      "         [254, 253, 253,  ..., 254, 254, 254],\n",
      "         [255, 255, 253,  ..., 253, 253, 253],\n",
      "         ...,\n",
      "         [199, 239, 255,  ..., 230, 231, 227],\n",
      "         [255, 255, 237,  ..., 255, 255, 252],\n",
      "         [255, 253, 255,  ..., 255, 255, 255]]], dtype=torch.uint8), 'label': tensor(0), 'data': tensor([0])}\n"
     ]
    }
   ],
   "source": [
    "for batch in val_data:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
